========,1,preface.
A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved ("hidden") states.
An HMM can be presented as the simplest dynamic Bayesian network.
The mathematics behind the HMM were developed by L. E. Baum and coworkers.
It is closely related to an earlier work on the optimal nonlinear filtering problem by Ruslan L. Stratonovich, who was the first to describe the forward-backward procedure.
In simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters.
In a "hidden" Markov model, the state is not directly visible, but the output, dependent on the state, is visible.
Each state has a probability distribution over the possible output tokens.
Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states.
The adjective 'hidden' refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a 'hidden' Markov model even if these parameters are known exactly.
Hidden Markov models are especially known for their application in temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.
A hidden Markov model can be considered a generalization of a mixture model where the hidden variables (or latent variables), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other.
Recently, hidden Markov models have been generalized to pairwise Markov models and triplet Markov models which allow consideration of more complex data structures and the modelling of nonstationary data.
========,2,Description in terms of urns.
In its discrete form, a hidden Markov process can be visualized as a generalization of the Urn problem with replacement (where each item from the urn is returned to the original urn before the next step).
Consider this example: in a room that is not visible to an observer there is a genie.
The room contains urns X1, X2, X3, … each of which contains a known mix of balls, each ball labeled y1, y2, y3, … .
The genie chooses an urn in that room and randomly draws a ball from that urn.
It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn.
The genie has some procedure to choose urns; the choice of the urn for the "n"-th ball depends only upon a random number and the choice of the urn for the ("n" − 1)-th ball.
The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a Markov process.
It can be described by the upper part of Figure 1.
The Markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a "hidden Markov process".
This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state.
Even if the observer knows the composition of the urns and has just observed a sequence of three balls, "e.g."
y1, y2 and y3 on the conveyor belt, the observer still cannot be "sure" which urn ("i.e.
", at which state) the genie has drawn the third ball from.
However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.
========,2,Inference.
========,3,Probability of an observed sequence.
The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence.
This requires summation over all possible state sequences:
The probability of observing a sequence of length "L" is given by where the sum runs over all possible hidden-node sequences Applying the principle of dynamic programming, this problem, too, can be handled efficiently using the forward algorithm.
========,3,Probability of the latent variables.
========,4,Filtering.
The task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e.
to compute ***formula***.
This task is normally used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points of time, with corresponding observations at each point in time.
Then, it is natural to ask about the state of the process at the end.
This problem can be handled efficiently using the forward algorithm.
========,4,Smoothing.
This is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e.
to compute ***formula*** for some ***formula***.
From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time "k" in the past, relative to time "t".
The forward-backward algorithm is an efficient method for computing the smoothed values for all hidden state variables.
========,4,Most likely explanation.
The task, unlike the previous two, asks about the joint probability of the "entire" sequence of hidden states that generated a particular sequence of observations (see illustration on the right).
This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable.
An example is part-of-speech tagging, where the hidden states represent the underlying parts of speech corresponding to an observed sequence of words.
In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.
This task requires finding a maximum over all possible state sequences, and can be solved efficiently by the Viterbi algorithm.
========,3,Statistical significance.
For some of the above problems, it may also be interesting to ask about statistical significance.
What is the probability that a sequence drawn from some null distribution will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence?
When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the false positive rate associated with failing to reject the hypothesis for the output sequence.
========,2,Learning.
The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities.
The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences.
No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum–Welch algorithm or the Baldi–Chauvin algorithm.
The Baum–Welch algorithm is a special case of the expectation-maximization algorithm.
If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability.
Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g.
Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.
========,2,Mathematical description.
========,3,Compared with a simple mixture model.
As mentioned above, the distribution of each observation in a hidden Markov model is a mixture density, with the states of the corresponding to mixture components.
It is useful to compare the above characterizations for an HMM with the corresponding characterizations, of a mixture model, using the same notation.
A non-Bayesian mixture model:
A Bayesian mixture model:
========,3,Poisson hidden Markov model.
"Poisson hidden Markov models (PHMM)" are special cases of hidden Markov models where a Poisson process has a rate which varies in association with changes between the different states of a Markov model.
PHMMs are not necessarily Markovian processes themselves because the underlying Markov chain or Markov process cannot be observed and only the Poisson signal is observed.
========,2,Applications.
HMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are).
Applications include:
***LIST***.
========,2,History.
The forward and backward recursions used in HMM as well as computations of marginal smoothing probabilities were first described by Ruslan L. Stratonovich in 1960 (pages 160—162) and in the late 1950s in his papers in Russian.
The Hidden Markov Models were later described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s.
One of the first applications of HMMs was speech recognition, starting in the mid-1970s.
In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences, in particular DNA.
Since then, they have become ubiquitous in the field of bioinformatics.
========,2,Types.
Hidden Markov models can model complex Markov processes where the states emit the observations according to some probability distribution.
One such example is the Gaussian distribution, in such a Hidden Markov Model the states output are represented by a Gaussian distribution.
Moreover, it could represent even more complex behavior when the output of the states is represented as mixture of two or more Gaussians, in which case the probability of generating an observation is the product of the probability of first selecting one of the Gaussians and the probability of generating that observation from that Gaussian.
In cases of modeled data exhibiting artifacts such as outliers and skewness, one may resort to finite mixtures of heavier-tailed elliptical distributions, such as the multivariate Student's-t distribution, or appropriate non-elliptical distributions, such as the multivariate Normal Inverse-Gaussian.
