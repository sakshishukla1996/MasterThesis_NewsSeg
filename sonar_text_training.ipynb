{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2001_10.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2001_11.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2001_12.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2001_9.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2002_1.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2002_10.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2002_11.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2002_12.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2002_2.jsonl'),\n",
       "  PosixPath('/home/operation/projects/data/2019_dw/dw-2019/de/2002_3.jsonl')],\n",
       " 814)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [ \"fr\", \"es\", \"de\",\"pt\"]\n",
    "# x = [\"hi\"]  # Removed due to sentence tokenization incapability.\n",
    "\n",
    "files = sorted(list(Path(\"/home/operation/projects/data/2019_dw/dw-2019/\").glob(\"**/*.jsonl\")))\n",
    "files = [i for i in files if i.parent.stem in x]\n",
    "files[:10], len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedURL</th>\n",
       "      <th>sourceItemType</th>\n",
       "      <th>sourceItemPageUrl</th>\n",
       "      <th>sourceItemIdAtOrigin</th>\n",
       "      <th>sourceItemOriginFeedName</th>\n",
       "      <th>sourceItemLangeCodeGuess</th>\n",
       "      <th>sourceItemTitle</th>\n",
       "      <th>sourceItemMainText</th>\n",
       "      <th>sourceItemTeaser</th>\n",
       "      <th>sourceItemKeywords</th>\n",
       "      <th>sourceItemPhotoURL</th>\n",
       "      <th>sourceItemDate</th>\n",
       "      <th>sourceItemScenarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/19WQ</td>\n",
       "      <td>http://api.dw.com/api/detail/article/274934</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Militärischer Aufmarsch rund um Afghanistan</td>\n",
       "      <td>Das Washingtoner Verteidigungsministerium hat ...</td>\n",
       "      <td>Die USA und Großbritannien ziehen nach den Ter...</td>\n",
       "      <td>[afghanistan, terror, usa, großbritannien, mil...</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-01 17:15:29</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/18t6</td>\n",
       "      <td>http://api.dw.com/api/detail/article/272496</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Blutbad in Schweizer Kanton Zug</td>\n",
       "      <td>Ein Amokläufer hat im Parlament des Schweizer ...</td>\n",
       "      <td>14 Tote bei Amoklauf in Schweizer Kantonsparla...</td>\n",
       "      <td>[amok, schweiz, zug, parlament, kanton]</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-02 11:14:51</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/19ZW</td>\n",
       "      <td>http://api.dw.com/api/detail/article/275126</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>\"Tag der offenen Moschee\"</td>\n",
       "      <td>Besonders nach den Terroranschlägen in den Ver...</td>\n",
       "      <td>Die 3,2 Millionen Muslime in Deutschland laden...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-02 11:13:34</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/19N0</td>\n",
       "      <td>http://api.dw.com/api/detail/article/274350</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>NATO sichert den USA Unterstützung zu</td>\n",
       "      <td>Die Mitgliedstaaten der Allianz haben die Wüns...</td>\n",
       "      <td>Die NATO-Verbündeten haben den USA Unterstützu...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-04 16:52:48</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/19rt</td>\n",
       "      <td>http://api.dw.com/api/detail/article/276265</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Mailänder Moderesümee</td>\n",
       "      <td>Die Defilees führten von Afrika (Jil Sander un...</td>\n",
       "      <td>Die ganze Welt ein Laufsteg: Mailands Designer...</td>\n",
       "      <td>[mailand, milano, sommermode, damenmode, kolle...</td>\n",
       "      <td>https://api.dw.com/image/275882_4.jpg</td>\n",
       "      <td>2001-10-04 15:53:56</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/1JX0</td>\n",
       "      <td>http://api.dw.com/api/detail/article/313410</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Flügellahmer Ferienflieger</td>\n",
       "      <td>Das Unternehmen versichert, dass der Flugbetri...</td>\n",
       "      <td>Schwarze Wolken, aber noch kein Gewittersturm ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-31 14:59:12</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/1JaQ</td>\n",
       "      <td>http://api.dw.com/api/detail/article/313622</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Das lange Warten auf den Beginn des Regierens</td>\n",
       "      <td>Was die neue bulgarische Regierung zweifellos ...</td>\n",
       "      <td>Eine Bilanz der ersten 100 Tage der neuen bulg...</td>\n",
       "      <td>[Bulgarien, Simeon, Regierung, Bilanz, Politik]</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-31 14:41:33</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/1Jla</td>\n",
       "      <td>http://api.dw.com/api/detail/article/314314</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Murren in Brüssel über Prodi</td>\n",
       "      <td>So richtig war es niemandem aufgefallen: Bei d...</td>\n",
       "      <td>Beim EU-Gipfel in Gent kam es hinter den Kulis...</td>\n",
       "      <td>[Prodi, EU, Belgien, Ratspräsidentschaft, Gent...</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-31 14:18:56</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/1JiD</td>\n",
       "      <td>http://api.dw.com/api/detail/article/314105</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>Baubeginn für Holocaust-Mahnmal in Berlin</td>\n",
       "      <td>Das deutsche Holocaust-Mahnmal wird im Herzen ...</td>\n",
       "      <td>Zehn Jahre lang wurde heftigst darüber debatti...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-31 11:53:42</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>http://api.dw.com</td>\n",
       "      <td>Article</td>\n",
       "      <td>https://p.dw.com/p/1JiG</td>\n",
       "      <td>http://api.dw.com/api/detail/article/314108</td>\n",
       "      <td>dw-history-feed</td>\n",
       "      <td>de</td>\n",
       "      <td>US-Hubschrauber unterstützen Nordallianz</td>\n",
       "      <td>Die Helikopter griffen am Mittwoch die Verteid...</td>\n",
       "      <td>US-Helikopter haben nach Angaben der Taliban d...</td>\n",
       "      <td>[Afghanistan, Taliban, Nordallianz, Bodentrupp...</td>\n",
       "      <td></td>\n",
       "      <td>2001-10-31 11:04:46</td>\n",
       "      <td>[DW, SUMMA]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               feedURL sourceItemType        sourceItemPageUrl  \\\n",
       "0    http://api.dw.com        Article  https://p.dw.com/p/19WQ   \n",
       "1    http://api.dw.com        Article  https://p.dw.com/p/18t6   \n",
       "2    http://api.dw.com        Article  https://p.dw.com/p/19ZW   \n",
       "3    http://api.dw.com        Article  https://p.dw.com/p/19N0   \n",
       "4    http://api.dw.com        Article  https://p.dw.com/p/19rt   \n",
       "..                 ...            ...                      ...   \n",
       "142  http://api.dw.com        Article  https://p.dw.com/p/1JX0   \n",
       "143  http://api.dw.com        Article  https://p.dw.com/p/1JaQ   \n",
       "144  http://api.dw.com        Article  https://p.dw.com/p/1Jla   \n",
       "145  http://api.dw.com        Article  https://p.dw.com/p/1JiD   \n",
       "146  http://api.dw.com        Article  https://p.dw.com/p/1JiG   \n",
       "\n",
       "                            sourceItemIdAtOrigin sourceItemOriginFeedName  \\\n",
       "0    http://api.dw.com/api/detail/article/274934          dw-history-feed   \n",
       "1    http://api.dw.com/api/detail/article/272496          dw-history-feed   \n",
       "2    http://api.dw.com/api/detail/article/275126          dw-history-feed   \n",
       "3    http://api.dw.com/api/detail/article/274350          dw-history-feed   \n",
       "4    http://api.dw.com/api/detail/article/276265          dw-history-feed   \n",
       "..                                           ...                      ...   \n",
       "142  http://api.dw.com/api/detail/article/313410          dw-history-feed   \n",
       "143  http://api.dw.com/api/detail/article/313622          dw-history-feed   \n",
       "144  http://api.dw.com/api/detail/article/314314          dw-history-feed   \n",
       "145  http://api.dw.com/api/detail/article/314105          dw-history-feed   \n",
       "146  http://api.dw.com/api/detail/article/314108          dw-history-feed   \n",
       "\n",
       "    sourceItemLangeCodeGuess                                sourceItemTitle  \\\n",
       "0                         de    Militärischer Aufmarsch rund um Afghanistan   \n",
       "1                         de                Blutbad in Schweizer Kanton Zug   \n",
       "2                         de                      \"Tag der offenen Moschee\"   \n",
       "3                         de          NATO sichert den USA Unterstützung zu   \n",
       "4                         de                          Mailänder Moderesümee   \n",
       "..                       ...                                            ...   \n",
       "142                       de                     Flügellahmer Ferienflieger   \n",
       "143                       de  Das lange Warten auf den Beginn des Regierens   \n",
       "144                       de                   Murren in Brüssel über Prodi   \n",
       "145                       de      Baubeginn für Holocaust-Mahnmal in Berlin   \n",
       "146                       de       US-Hubschrauber unterstützen Nordallianz   \n",
       "\n",
       "                                    sourceItemMainText  \\\n",
       "0    Das Washingtoner Verteidigungsministerium hat ...   \n",
       "1    Ein Amokläufer hat im Parlament des Schweizer ...   \n",
       "2    Besonders nach den Terroranschlägen in den Ver...   \n",
       "3    Die Mitgliedstaaten der Allianz haben die Wüns...   \n",
       "4    Die Defilees führten von Afrika (Jil Sander un...   \n",
       "..                                                 ...   \n",
       "142  Das Unternehmen versichert, dass der Flugbetri...   \n",
       "143  Was die neue bulgarische Regierung zweifellos ...   \n",
       "144  So richtig war es niemandem aufgefallen: Bei d...   \n",
       "145  Das deutsche Holocaust-Mahnmal wird im Herzen ...   \n",
       "146  Die Helikopter griffen am Mittwoch die Verteid...   \n",
       "\n",
       "                                      sourceItemTeaser  \\\n",
       "0    Die USA und Großbritannien ziehen nach den Ter...   \n",
       "1    14 Tote bei Amoklauf in Schweizer Kantonsparla...   \n",
       "2    Die 3,2 Millionen Muslime in Deutschland laden...   \n",
       "3    Die NATO-Verbündeten haben den USA Unterstützu...   \n",
       "4    Die ganze Welt ein Laufsteg: Mailands Designer...   \n",
       "..                                                 ...   \n",
       "142  Schwarze Wolken, aber noch kein Gewittersturm ...   \n",
       "143  Eine Bilanz der ersten 100 Tage der neuen bulg...   \n",
       "144  Beim EU-Gipfel in Gent kam es hinter den Kulis...   \n",
       "145  Zehn Jahre lang wurde heftigst darüber debatti...   \n",
       "146  US-Helikopter haben nach Angaben der Taliban d...   \n",
       "\n",
       "                                    sourceItemKeywords  \\\n",
       "0    [afghanistan, terror, usa, großbritannien, mil...   \n",
       "1              [amok, schweiz, zug, parlament, kanton]   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4    [mailand, milano, sommermode, damenmode, kolle...   \n",
       "..                                                 ...   \n",
       "142                                                 []   \n",
       "143    [Bulgarien, Simeon, Regierung, Bilanz, Politik]   \n",
       "144  [Prodi, EU, Belgien, Ratspräsidentschaft, Gent...   \n",
       "145                                                 []   \n",
       "146  [Afghanistan, Taliban, Nordallianz, Bodentrupp...   \n",
       "\n",
       "                        sourceItemPhotoURL       sourceItemDate  \\\n",
       "0                                           2001-10-01 17:15:29   \n",
       "1                                           2001-10-02 11:14:51   \n",
       "2                                           2001-10-02 11:13:34   \n",
       "3                                           2001-10-04 16:52:48   \n",
       "4    https://api.dw.com/image/275882_4.jpg  2001-10-04 15:53:56   \n",
       "..                                     ...                  ...   \n",
       "142                                         2001-10-31 14:59:12   \n",
       "143                                         2001-10-31 14:41:33   \n",
       "144                                         2001-10-31 14:18:56   \n",
       "145                                         2001-10-31 11:53:42   \n",
       "146                                         2001-10-31 11:04:46   \n",
       "\n",
       "    sourceItemScenarios  \n",
       "0           [DW, SUMMA]  \n",
       "1           [DW, SUMMA]  \n",
       "2           [DW, SUMMA]  \n",
       "3           [DW, SUMMA]  \n",
       "4           [DW, SUMMA]  \n",
       "..                  ...  \n",
       "142         [DW, SUMMA]  \n",
       "143         [DW, SUMMA]  \n",
       "144         [DW, SUMMA]  \n",
       "145         [DW, SUMMA]  \n",
       "146         [DW, SUMMA]  \n",
       "\n",
       "[147 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(files[0], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172]) torch.Size([172, 1024])\n",
      "torch.Size([167]) torch.Size([167, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([167, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(list(Path(\"/home/operation/projects/notebooks/validation/\").glob(\"**/*.jsonl\")))\n",
    "for file in files:\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    text = df['sourceItemMainText']\n",
    "    tokens, labels = tokenize_document(text)\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedder.predict(tokens, source_lang=\"eng_Latn\", batch_size=64)\n",
    "    save_data = {\n",
    "        \"sentences\": tokens,\n",
    "        \"embeddings\": embeddings.detach().cpu(),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "    }\n",
    "    print(torch.tensor(labels).shape, embeddings.shape)\n",
    "    torch.save(save_data, f\"/home/operation/projects/notebooks/validation/{file.stem}.pt\")\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[-0.0057,  0.0184, -0.0117,  ..., -0.0051, -0.0055, -0.0093],\n",
       "         [-0.0034,  0.0215, -0.0080,  ..., -0.0008, -0.0006,  0.0042],\n",
       "         [ 0.0015,  0.0009, -0.0037,  ..., -0.0074, -0.0071,  0.0056],\n",
       "         ...,\n",
       "         [ 0.0053,  0.0101,  0.0044,  ..., -0.0150, -0.0006,  0.0036],\n",
       "         [-0.0040, -0.0020, -0.0039,  ..., -0.0171, -0.0093, -0.0123],\n",
       "         [-0.0040, -0.0056, -0.0048,  ...,  0.0042,  0.0025,  0.0107]],\n",
       "        device='cuda:0'),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"/home/operation/projects/notebooks/validation/val_ndtv_sports.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedURL                                                     http://api.dw.com\n",
       "sourceItemType                                                        Article\n",
       "sourceItemPageUrl                                     https://p.dw.com/p/19WQ\n",
       "sourceItemIdAtOrigin              http://api.dw.com/api/detail/article/274934\n",
       "sourceItemOriginFeedName                                      dw-history-feed\n",
       "sourceItemLangeCodeGuess                                                   de\n",
       "sourceItemTitle                   Militärischer Aufmarsch rund um Afghanistan\n",
       "sourceItemMainText          Das Washingtoner Verteidigungsministerium hat ...\n",
       "sourceItemTeaser            Die USA und Großbritannien ziehen nach den Ter...\n",
       "sourceItemKeywords          [afghanistan, terror, usa, großbritannien, mil...\n",
       "sourceItemPhotoURL                                                           \n",
       "sourceItemDate                                            2001-10-01 17:15:29\n",
       "sourceItemScenarios                                               [DW, SUMMA]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(files[0], lines=True)\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 167)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text: str, lang: str=\"en\"):\n",
    "    \"\"\"\n",
    "    Improve this logic later for multiple languages\n",
    "    \"\"\"\n",
    "    nltk_tokenizer_lang = [\"en\", \"fr\", \"es\", \"de\", \"pt\"]\n",
    "    if lang in nltk_tokenizer_lang:\n",
    "        tokens = sent_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_document(list_docs: list, lang=\"en\"):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for t in list_docs:\n",
    "        toks = tokenizer(t, lang=lang)\n",
    "        tokens+=toks\n",
    "        labels+=[0]*len(toks)\n",
    "        labels[-1]=1\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "# tokens = tokenizer(df.iloc[0][\"sourceItemMainText\"], lang=\"en\")\n",
    "# labels = [0] * len(tokens)\n",
    "# labels[-1] = 1\n",
    "# labels\n",
    "\n",
    "toks, labs = tokenize_document(df['sourceItemMainText'].tolist(), lang=\"en\")\n",
    "len(toks), len(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonar.models.sonar_speech.loader import load_sonar_speech_model\n",
    "from sonar.models.sonar_text import (\n",
    "    load_sonar_text_decoder_model,\n",
    "    load_sonar_text_encoder_model,\n",
    "    load_sonar_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of text_sonar_basic_encoder. Set `force` to `True` to download again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_symbols=['__ace_Arab__', '__ace_Latn__', '__acm_Arab__', '__acq_Arab__', '__aeb_Arab__', '__afr_Latn__', '__ajp_Arab__', '__aka_Latn__', '__amh_Ethi__', '__apc_Arab__', '__arb_Arab__', '__ars_Arab__', '__ary_Arab__', '__arz_Arab__', '__asm_Beng__', '__ast_Latn__', '__awa_Deva__', '__ayr_Latn__', '__azb_Arab__', '__azj_Latn__', '__bak_Cyrl__', '__bam_Latn__', '__ban_Latn__', '__bel_Cyrl__', '__bem_Latn__', '__ben_Beng__', '__bho_Deva__', '__bjn_Arab__', '__bjn_Latn__', '__bod_Tibt__', '__bos_Latn__', '__bug_Latn__', '__bul_Cyrl__', '__cat_Latn__', '__ceb_Latn__', '__ces_Latn__', '__cjk_Latn__', '__ckb_Arab__', '__crh_Latn__', '__cym_Latn__', '__dan_Latn__', '__deu_Latn__', '__dik_Latn__', '__dyu_Latn__', '__dzo_Tibt__', '__ell_Grek__', '__eng_Latn__', '__epo_Latn__', '__est_Latn__', '__eus_Latn__', '__ewe_Latn__', '__fao_Latn__', '__pes_Arab__', '__fij_Latn__', '__fin_Latn__', '__fon_Latn__', '__fra_Latn__', '__fur_Latn__', '__fuv_Latn__', '__gla_Latn__', '__gle_Latn__', '__glg_Latn__', '__grn_Latn__', '__guj_Gujr__', '__hat_Latn__', '__hau_Latn__', '__heb_Hebr__', '__hin_Deva__', '__hne_Deva__', '__hrv_Latn__', '__hun_Latn__', '__hye_Armn__', '__ibo_Latn__', '__ilo_Latn__', '__ind_Latn__', '__isl_Latn__', '__ita_Latn__', '__jav_Latn__', '__jpn_Jpan__', '__kab_Latn__', '__kac_Latn__', '__kam_Latn__', '__kan_Knda__', '__kas_Arab__', '__kas_Deva__', '__kat_Geor__', '__knc_Arab__', '__knc_Latn__', '__kaz_Cyrl__', '__kbp_Latn__', '__kea_Latn__', '__khm_Khmr__', '__kik_Latn__', '__kin_Latn__', '__kir_Cyrl__', '__kmb_Latn__', '__kon_Latn__', '__kor_Hang__', '__kmr_Latn__', '__lao_Laoo__', '__lvs_Latn__', '__lij_Latn__', '__lim_Latn__', '__lin_Latn__', '__lit_Latn__', '__lmo_Latn__', '__ltg_Latn__', '__ltz_Latn__', '__lua_Latn__', '__lug_Latn__', '__luo_Latn__', '__lus_Latn__', '__mag_Deva__', '__mai_Deva__', '__mal_Mlym__', '__mar_Deva__', '__min_Latn__', '__mkd_Cyrl__', '__plt_Latn__', '__mlt_Latn__', '__mni_Beng__', '__khk_Cyrl__', '__mos_Latn__', '__mri_Latn__', '__zsm_Latn__', '__mya_Mymr__', '__nld_Latn__', '__nno_Latn__', '__nob_Latn__', '__npi_Deva__', '__nso_Latn__', '__nus_Latn__', '__nya_Latn__', '__oci_Latn__', '__gaz_Latn__', '__ory_Orya__', '__pag_Latn__', '__pan_Guru__', '__pap_Latn__', '__pol_Latn__', '__por_Latn__', '__prs_Arab__', '__pbt_Arab__', '__quy_Latn__', '__ron_Latn__', '__run_Latn__', '__rus_Cyrl__', '__sag_Latn__', '__san_Deva__', '__sat_Beng__', '__scn_Latn__', '__shn_Mymr__', '__sin_Sinh__', '__slk_Latn__', '__slv_Latn__', '__smo_Latn__', '__sna_Latn__', '__snd_Arab__', '__som_Latn__', '__sot_Latn__', '__spa_Latn__', '__als_Latn__', '__srd_Latn__', '__srp_Cyrl__', '__ssw_Latn__', '__sun_Latn__', '__swe_Latn__', '__swh_Latn__', '__szl_Latn__', '__tam_Taml__', '__tat_Cyrl__', '__tel_Telu__', '__tgk_Cyrl__', '__tgl_Latn__', '__tha_Thai__', '__tir_Ethi__', '__taq_Latn__', '__taq_Tfng__', '__tpi_Latn__', '__tsn_Latn__', '__tso_Latn__', '__tuk_Latn__', '__tum_Latn__', '__tur_Latn__', '__twi_Latn__', '__tzm_Tfng__', '__uig_Arab__', '__ukr_Cyrl__', '__umb_Latn__', '__urd_Arab__', '__uzn_Latn__', '__vec_Latn__', '__vie_Latn__', '__war_Latn__', '__wol_Latn__', '__xho_Latn__', '__ydd_Hebr__', '__yor_Latn__', '__yue_Hant__', '__zho_Hans__', '__zho_Hant__', '__zul_Latn__']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached tokenizer of text_sonar_basic_encoder. Set `force` to `True` to download again.\n"
     ]
    }
   ],
   "source": [
    "t2enc = load_sonar_text_encoder_model(\"text_sonar_basic_encoder\", device=device).eval()\n",
    "text_tokenizer = load_sonar_tokenizer(\"text_sonar_basic_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TextToEmbeddingModelPipeline(t2enc, text_tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mtoks\u001b[49m, source_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng_Latn\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'toks' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = embedder.predict(toks, source_lang=\"eng_Latn\", batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3878, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, encoder=\"text_sonar_basic_encoder\", tokenizer=\"text_sonar_basic_encoder\", device=\"cuda\") -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(device, str): \n",
    "            device = torch.device(device)\n",
    "        t2enc = load_sonar_text_encoder_model(encoder, device=device).eval()\n",
    "        text_tokenizer = load_sonar_tokenizer(tokenizer)\n",
    "        self.backbone = TextToEmbeddingModelPipeline(t2enc, text_tokenizer, device=device)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Linear(1024, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 2)\n",
    "                    )\n",
    "        self.head.to(device)\n",
    "\n",
    "    def forward(self, sentence_list, source_lang=\"eng_Latn\"):\n",
    "        embeddings = self.backbone.predict(sentence_list, source_lang=source_lang, batch_size=64)\n",
    "        predictions = self.head(embeddings)\n",
    "        return predictions\n",
    "\n",
    "class MyModel2(nn.Module):\n",
    "    def __init__(self, device=\"cuda\") -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(device, str): \n",
    "            device = torch.device(device)\n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Linear(1024, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 2)\n",
    "                    )\n",
    "        self.head.to(device)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        predictions = self.head(embeddings)\n",
    "        return predictions\n",
    "model = MyModel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3878, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = embedder.predict(toks, source_lang=\"eng_Latn\", batch_size=64)\n",
    "    out = model(embeddings)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str, lang: str=\"en\"):\n",
    "    \"\"\"\n",
    "    Improve this logic later for multiple languages\n",
    "    \"\"\"\n",
    "    nltk_tokenizer_lang = [\"en\", \"fr\", \"es\", \"de\", \"pt\"]\n",
    "    if lang in nltk_tokenizer_lang:\n",
    "        tokens = sent_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_document(list_docs: list, lang=\"en\"):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for t in list_docs:\n",
    "        toks = tokenizer(t, lang=lang)\n",
    "        tokens+=toks\n",
    "        labels+=[0]*len(toks)\n",
    "        labels[-1]=1\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, languages: list=[\"en\", \"fr\", \"es\", \"de\", \"pt\"]):\n",
    "        self.data_dir = data_dir\n",
    "        self.languages = languages\n",
    "        files = sorted(list(Path(self.data_dir).glob(\"**/*.jsonl\")))\n",
    "        self.files = [i for i in files if i.parent.stem in self.languages]\n",
    "        print(f\"Found {len(self.files)} files in {self.data_dir} having following langagues: {set([i.parent.stem for i in self.files])}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_json(self.files[idx], lines=True)\n",
    "        x, y = tokenize_document(df['sourceItemMainText'].tolist(), lang=\"en\") \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 files in /home/operation/projects/data/2019_dw/dw-2019 having following langagues: {'en', 'es', 'pt', 'fr', 'de'}\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(data_dir=\"/home/operation/projects/data/2019_dw/dw-2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6858, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ,y = dataset[1]\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788482"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataset))\n",
    "inp, labl = data\n",
    "with torch.no_grad():\n",
    "    embeddings = embedder.predict(inp, source_lang=\"eng_Latn\", batch_size=64)\n",
    "\n",
    "embb = torch.tensor(embeddings)\n",
    "outputs = model(embb)\n",
    "outputs.shape, labl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(outputs, torch.tensor(labl).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, data in enumerate(dataset):\n",
    "    inp, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedder.predict(inp, source_lang=\"eng_Latn\", batch_size=64)\n",
    "        embb = torch.tensor(embeddings)\n",
    "    outputs = model(embb)\n",
    "    loss = loss_fn(outputs, torch.tensor(labl).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if i % 10 == 0:\n",
    "        last_loss = running_loss / 1000 # loss per batch\n",
    "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "        tb_x = epoch_index * len(training_loader) + i + 1\n",
    "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "        running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
